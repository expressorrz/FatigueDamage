{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "def normalize_images(images):\n",
    "    \"\"\"\n",
    "        Normalizes images between zero and a predefined maximum value\n",
    "\n",
    "        Parameters:\n",
    "            images: torch.Tensor\n",
    "                input images to be normalized\n",
    "            normalize_value_dataset:\n",
    "                maximum value to which the images shall be normalized\n",
    "\n",
    "        Returns:\n",
    "            images: torch.Tensor\n",
    "                normalized images\n",
    "    \"\"\"\n",
    "    normalize_value_dataset = 255\n",
    "    images = images - torch.amin(images, dim=(-2, -1)).unsqueeze(dim=-1).unsqueeze(dim=-1)\n",
    "    images = torch.div(images, torch.amax(images, dim=(-2, -1)).unsqueeze(dim=-1).unsqueeze(dim=-1))\n",
    "    images = (normalize_value_dataset * images).to(torch.uint8)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(i, resize):\n",
    "    data_list = []\n",
    "    print(f'loading A{i:03}...')\n",
    "    remaining_life_list = []\n",
    "    capture_time_list = []\n",
    "    load_path = f'../rename_dataset/A{i:03}'\n",
    "    files = glob.glob(f'{load_path}*.tiff')\n",
    "    # print(files)\n",
    "    for file in files:\n",
    "        # camera_id, remaining_life, capture_time = file.split('_')[1:4]\n",
    "        file_name = file[:-5].split(\"_\")\n",
    "        camera_id, remaining_life, capture_time = file_name[-3], file_name[-2], file_name[-1]\n",
    "        remaining_life_list.append(int(remaining_life))\n",
    "        capture_time_list.append(int(capture_time))\n",
    "\n",
    "    data_array = np.concatenate([np.array(remaining_life_list).reshape(-1, 1), np.array(capture_time_list).reshape(-1, 1)], axis=1)\n",
    "    \n",
    "    data_array = data_array[np.lexsort((data_array[:, 1], data_array[:, 0]))]\n",
    "    _, idx = np.unique(data_array[:, 0], return_index=True)\n",
    "    data_array = data_array[idx][::-1]\n",
    "    \n",
    "    # Exclude the first 8/9 (even number rqrd) images because then the experiment still has to stabilise\n",
    "    if data_array.shape[0] % 2 == 0:\n",
    "        data_array = data_array[8:]\n",
    "    else:\n",
    "        data_array = data_array[9:]\n",
    "    \n",
    "\n",
    "\n",
    "    # print(data_array.shape)\n",
    "\n",
    "    for idx, v in enumerate(data_array):\n",
    "        # Exclude RUL < 4000 (otherwise large bias)\n",
    "        if v[0] < 4000:\n",
    "            tiff_path_0 = f'{load_path}_0_{v[0]}_{v[1]}.tiff'\n",
    "            tiff_path_1 = f'{load_path}_1_{v[0]}_{v[1]}.tiff'\n",
    "            \n",
    "            src_0 = torch.squeeze(resize(torch.unsqueeze(torch.Tensor(np.array(Image.open(tiff_path_0))), 0))).to(torch.uint8).unsqueeze(0)\n",
    "            src_1 = torch.squeeze(resize(torch.unsqueeze(torch.Tensor(np.array(Image.open(tiff_path_1))), 0))).to(torch.uint8).unsqueeze(0)\n",
    "            src = torch.cat((src_0, src_1), 0)\n",
    "            src = np.array(src)\n",
    "\n",
    "            data_list.append([src, int(v[0]), i])\n",
    "\n",
    "    # x = np.array([data[0] for data in data_list]) \n",
    "    x = np.array([data[0] for data in data_list])\n",
    "    y = np.array([data[1] for data in data_list])\n",
    "\n",
    "    # Apply histogram equalization and normalize images between 0 and normalize value dataset\n",
    "    # x = normalize_images(torch.Tensor(x))\n",
    "    # x = np.array(x)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import tqdm\n",
    "def augment_data(i, train_x, train_y, N, cpu_storage=True, sigma_rul=0.01):\n",
    "    \"\"\"\n",
    "        Function to create an augmented dataset\n",
    "        It is saved with name filename_dataset_training_set_augmented, and only augments the training set\n",
    "        \n",
    "        Args:\n",
    "            filename_dataset: string\n",
    "                filename of the dataset without extension (e.g. window_size_7_test_0)\n",
    "            N: int\n",
    "                number of times you want to augment the entire dataset with the same (however randomized) augmentations\n",
    "            cpu_storage: bool\n",
    "                if True, stores the dataset on the cpu, else it is stored on cuda if available\n",
    "            sigma_rul: float\n",
    "                standard deviation of the noise added to the RUL\n",
    "    \"\"\"\n",
    "    # Initialize devices\n",
    "    devices = torch.device('cpu') if cpu_storage else torch.device('cuda')\n",
    "    # Load dataset\n",
    "    image_width_aug = 320\n",
    "    image_height_aug = 640\n",
    "\n",
    "    # Antialias parameter is not available in torchvision version smaller than or equal to 13\n",
    "    torchvision_version, torchvision_subversion = torchvision.__version__.split('.')[:2]\n",
    "    torchvision_version, torchvision_subversion = int(torchvision_version), int(torchvision_subversion)\n",
    "    if torchvision_version == 0 and torchvision_subversion <= 13:\n",
    "        resized_crop = T.RandomResizedCrop((image_height_aug, image_width_aug), scale=(0.5, 1.0), ratio=(7/8, 8/7))\n",
    "    else:\n",
    "        resized_crop = T.RandomResizedCrop((image_height_aug, image_width_aug), scale=(0.5, 1.0), ratio=(7/8, 8/7), antialias=True)\n",
    "\n",
    "    # Initialize the (type of) augmentations\n",
    "    augmentations = T.Compose([\n",
    "        resized_crop,\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomVerticalFlip(p=0.5),\n",
    "        T.RandomRotation([-5, 5]),\n",
    "    ])\n",
    "\n",
    "    # Initialize list\n",
    "    train_x_augmented = []\n",
    "    train_y_augmented = []\n",
    "\n",
    "    # Loop through the dataset for N times to apply augmentations\n",
    "    current_len = 0\n",
    "    for _ in tqdm.tqdm(range(N), leave=True):\n",
    "        train_x_current = train_x.detach()\n",
    "        train_y_current = train_y.detach()\n",
    "        # temp = normalize_images(augmentations(train_x_current)).cpu()\n",
    "        # print('temp:', temp.shape)\n",
    "        train_x_augmented.append(normalize_images(augmentations(train_x_current)).cpu())\n",
    "        train_y_augmented.append(train_y_current)\n",
    "\n",
    "        del train_x_current\n",
    "        del train_y_current\n",
    "\n",
    "    # Concatenate all the lists to a tensor\n",
    "    train_x_augmented = torch.cat(train_x_augmented)\n",
    "    train_y_augmented = torch.cat(train_y_augmented)\n",
    "    print('train_x_augmented:', train_x_augmented.shape, 'train_y_augmented:', train_y_augmented.shape)\n",
    "\n",
    "    # Save the data\n",
    "    os.makedirs(f'../dataset_aug/A{i:03}', exist_ok=True)\n",
    "    np.save(f'../dataset_aug/A{i:03}/x.npy', train_x_augmented)\n",
    "    np.save(f'../dataset_aug/A{i:03}/y.npy', train_y_augmented)\n",
    "\n",
    "    del train_x_augmented\n",
    "    del train_y_augmented\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading A003...\n",
      "A003 x shape: (56, 2, 640, 320), y shape: (56,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([560, 2, 640, 320]) train_y_augmented: torch.Size([560])\n",
      "A003 done\n",
      "loading A009...\n",
      "A009 x shape: (64, 2, 640, 320), y shape: (64,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 10.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([640, 2, 640, 320]) train_y_augmented: torch.Size([640])\n",
      "A009 done\n",
      "loading A011...\n",
      "A011 x shape: (92, 2, 640, 320), y shape: (92,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([920, 2, 640, 320]) train_y_augmented: torch.Size([920])\n",
      "A011 done\n",
      "loading A012...\n",
      "A012 x shape: (56, 2, 640, 320), y shape: (56,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([560, 2, 640, 320]) train_y_augmented: torch.Size([560])\n",
      "A012 done\n",
      "loading A013...\n",
      "A013 x shape: (50, 2, 640, 320), y shape: (50,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([500, 2, 640, 320]) train_y_augmented: torch.Size([500])\n",
      "A013 done\n",
      "loading A014...\n",
      "A014 x shape: (84, 2, 640, 320), y shape: (84,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_augmented: torch.Size([840, 2, 640, 320]) train_y_augmented: torch.Size([840])\n",
      "A014 done\n"
     ]
    }
   ],
   "source": [
    "source = [3, 9, 11, 12, 13, 14]\n",
    "\n",
    "# source = [3]\n",
    "\n",
    "resize = torchvision.transforms.Resize((640, 320), antialias=True)\n",
    "\n",
    "for i in source:\n",
    "    os.makedirs(f'../dataset/A{i:03}', exist_ok=True)\n",
    "    x, y = load_source(i, resize)\n",
    "    print(f'A{i:03} x shape: {x.shape}, y shape: {y.shape}')\n",
    "    np.save(f'../dataset/A{i:03}/x.npy', x)\n",
    "    np.save(f'../dataset/A{i:03}/y.npy', y)\n",
    "\n",
    "    augment_data(i, torch.Tensor(x), torch.Tensor(y), 20)\n",
    "\n",
    "    print(f'A{i:03} done')\n",
    "\n",
    "# num = sum([np.load(f'../dataset/A{i:03}/x.npy').shape[0] for i in source])\n",
    "# print(f'total: {num}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
